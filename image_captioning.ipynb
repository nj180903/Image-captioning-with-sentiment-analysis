{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXCx58nKabs8"
      },
      "outputs": [],
      "source": [
        "# @title Default title text\n",
        "!pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nEdAGjNFahZ5"
      },
      "outputs": [],
      "source": [
        "# Upload kaggle.json\n",
        "from google.colab import files\n",
        "files.upload()  # This will prompt you to upload the kaggle.json file\n",
        "\n",
        "# Create the .kaggle directory\n",
        "import os\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Move the kaggle.json file to the .kaggle directory\n",
        "import shutil\n",
        "shutil.move(\"kaggle.json\", \"/root/.kaggle/kaggle.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZuwRRXGak92"
      },
      "outputs": [],
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Download the dataset\n",
        "!kaggle datasets download -d adityajn105/flickr8k\n",
        "\n",
        "# Unzip the dataset\n",
        "!unzip flickr8k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtbLjiRJclfr"
      },
      "outputs": [],
      "source": [
        "pip install textblob\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Wy8CsdXcq4W"
      },
      "outputs": [],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y12Ia4LwanJe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence, to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Reshape, Embedding, concatenate, LSTM, Dropout, add\n",
        "from tensorflow.keras.applications import DenseNet201\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textwrap import wrap\n",
        "from textblob import TextBlob\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0HCKo10c3N0"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_style(\"dark\")\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBE-Rdsmap5j"
      },
      "outputs": [],
      "source": [
        "image_path = '/content/Images'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slWM-rrKaryT"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/content/captions.txt\")\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlTbWrlPatpT"
      },
      "outputs": [],
      "source": [
        "def readImage(path, img_size=224):\n",
        "    img = load_img(path, color_mode='rgb', target_size=(img_size, img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.\n",
        "    return img\n",
        "\n",
        "# Function to display images with captions\n",
        "def display_images(temp_df):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    n = 0\n",
        "    for i in range(15):\n",
        "        n += 1\n",
        "        plt.subplot(5, 5, n)\n",
        "        plt.subplots_adjust(hspace=0.7, wspace=0.3)\n",
        "        image = readImage(f\"/content/Images/{temp_df.image[i]}\")\n",
        "        plt.imshow(image)\n",
        "        plt.title(\"\\n\".join(wrap(temp_df.caption[i], 20)))\n",
        "        plt.axis(\"off\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcIm0W5LauQF"
      },
      "outputs": [],
      "source": [
        "def text_preprocessing(data):\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.lower())\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"[^A-Za-z]\", \"\"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: x.replace(\"\\s+\", \" \"))\n",
        "    data['caption'] = data['caption'].apply(lambda x: \" \".join([word for word in x.split() if len(word) > 1]))\n",
        "    data['caption'] = \"startseq \" + data['caption'] + \" endseq\"\n",
        "    return data\n",
        "\n",
        "data = text_preprocessing(data)\n",
        "captions = data['caption'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZxcIDSuAawDz"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "max_length = max(len(caption.split()) for caption in captions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3WB_LI2dTvJ"
      },
      "outputs": [],
      "source": [
        "# Splitting data\n",
        "images = data['image'].unique().tolist()\n",
        "nimages = len(images)\n",
        "split_index = round(0.85 * nimages)\n",
        "train_images = images[:split_index]\n",
        "val_images = images[split_index:]\n",
        "\n",
        "train = data[data['image'].isin(train_images)]\n",
        "test = data[data['image'].isin(val_images)]\n",
        "train.reset_index(inplace=True, drop=True)\n",
        "test.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyz8PQohdW-7",
        "outputId": "9fede485-05d4-4c12-a0f1-e29f5a72189b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet201_weights_tf_dim_ordering_tf_kernels.h5\n",
            "82524592/82524592 [==============================] - 2s 0us/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 8091/8091 [45:48<00:00,  2.94it/s]\n"
          ]
        }
      ],
      "source": [
        "model = DenseNet201()\n",
        "fe = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "\n",
        "img_size = 224\n",
        "features = {}\n",
        "for image in tqdm(data['image'].unique().tolist()):\n",
        "    img = load_img(os.path.join(image_path,image),target_size=(img_size,img_size))\n",
        "    img = img_to_array(img)\n",
        "    img = img/255.\n",
        "    img = np.expand_dims(img,axis=0)\n",
        "    feature = fe.predict(img, verbose=0)\n",
        "    features[image] = feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a81xnccldeCt"
      },
      "outputs": [],
      "source": [
        "# Custom data generator\n",
        "class CustomDataGenerator(Sequence):\n",
        "    def __init__(self, df, X_col, y_col, batch_size, directory, tokenizer,\n",
        "                 vocab_size, max_length, features, shuffle=True):\n",
        "        self.df = df.copy()\n",
        "        self.X_col = X_col\n",
        "        self.y_col = y_col\n",
        "        self.directory = directory\n",
        "        self.batch_size = batch_size\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_length = max_length\n",
        "        self.features = features\n",
        "        self.shuffle = shuffle\n",
        "        self.n = len(self.df)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        if self.shuffle:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n // self.batch_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch = self.df.iloc[index * self.batch_size:(index + 1) * self.batch_size, :]\n",
        "        X1, X2, y = self.__get_data(batch)\n",
        "        return (X1, X2), y\n",
        "\n",
        "    def __get_data(self, batch):\n",
        "        X1, X2, y = list(), list(), list()\n",
        "        images = batch[self.X_col].tolist()\n",
        "        for image in images:\n",
        "            feature = self.features[image][0]\n",
        "            captions = batch.loc[batch[self.X_col] == image, self.y_col].tolist()\n",
        "            for caption in captions:\n",
        "                seq = self.tokenizer.texts_to_sequences([caption])[0]\n",
        "                for i in range(1, len(seq)):\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
        "                    out_seq = to_categorical([out_seq], num_classes=self.vocab_size)[0]\n",
        "                    X1.append(feature)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "        X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "        return X1, X2, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QWFlERJdsqY"
      },
      "outputs": [],
      "source": [
        "# Model definition\n",
        "input1 = Input(shape=(1920,))\n",
        "input2 = Input(shape=(max_length,))\n",
        "img_features = Dense(256, activation='relu')(input1)\n",
        "img_features_reshaped = Reshape((1, 256), input_shape=(256,))(img_features)\n",
        "sentence_features = Embedding(vocab_size, 256, mask_zero=False)(input2)\n",
        "merged = concatenate([img_features_reshaped, sentence_features], axis=1)\n",
        "sentence_features = LSTM(256)(merged)\n",
        "x = Dropout(0.5)(sentence_features)\n",
        "x = add([x, img_features])\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(vocab_size, activation='softmax')(x)\n",
        "caption_model = Model(inputs=[input1, input2], outputs=output)\n",
        "caption_model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YaNVPI5kdy3A"
      },
      "outputs": [],
      "source": [
        "# Plot model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(caption_model)\n",
        "caption_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8XAXj_Dd1zp"
      },
      "outputs": [],
      "source": [
        "# Generators\n",
        "train_generator = CustomDataGenerator(df=train, X_col='image', y_col='caption', batch_size=64, directory=image_path,\n",
        "                                      tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, features=features)\n",
        "validation_generator = CustomDataGenerator(df=test, X_col='image', y_col='caption', batch_size=64, directory=image_path,\n",
        "                                           tokenizer=tokenizer, vocab_size=vocab_size, max_length=max_length, features=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4KV44HTrkJq"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Callbacks\n",
        "model_name = \"model.h5\"\n",
        "checkpoint = ModelCheckpoint(model_name, monitor=\"val_loss\", mode=\"min\", save_best_only=True, verbose=1)\n",
        "earlystopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=1, restore_best_weights=True)\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.2, min_lr=1e-8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9gPfI9nd8LF"
      },
      "outputs": [],
      "source": [
        "# Training the model\n",
        "history = caption_model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    callbacks=[checkpoint, earlystopping, learning_rate_reduction]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-77QrkABXOb"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Initialize references dictionary\n",
        "references = {}\n",
        "\n",
        "# Batch size for processing\n",
        "batch_size = 100\n",
        "\n",
        "# Initialize lists for storing predictions and references\n",
        "predicted_captions = []\n",
        "\n",
        "# Process batches of data\n",
        "for start_idx in range(0, len(test), batch_size):\n",
        "    end_idx = min(start_idx + batch_size, len(test))\n",
        "    batch = test.iloc[start_idx:end_idx]\n",
        "\n",
        "    # Generate predictions for the batch\n",
        "    batch_predictions = []\n",
        "    for image in batch['image']:\n",
        "        predicted_caption = predict_caption(caption_model, image, tokenizer, max_length, features)\n",
        "        batch_predictions.append(predicted_caption)\n",
        "    predicted_captions.extend(batch_predictions)\n",
        "\n",
        "    # Prepare reference captions for the batch\n",
        "    for index, row in batch.iterrows():\n",
        "        img_name = row['image']\n",
        "        if img_name not in references:\n",
        "            references[img_name] = []\n",
        "        references[img_name].append(row['caption'].split())\n",
        "\n",
        "# Convert references to the format expected by nltk corpus_bleu\n",
        "reference_bleu = [[captions] for _, captions in references.items()]\n",
        "predicted_bleu = [caption.split() for caption in predicted_captions]\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_score = corpus_bleu(reference_bleu, predicted_bleu)\n",
        "print(f\"BLEU Score: {bleu_score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGyBBYTBeAES"
      },
      "outputs": [],
      "source": [
        "# Plotting the training history\n",
        "plt.figure(figsize=(20, 8))\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kw_2KkoyeC7S"
      },
      "outputs": [],
      "source": [
        "# Function to map indices to words\n",
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mF0z5Jq9eFZW"
      },
      "outputs": [],
      "source": [
        "# Function to predict caption\n",
        "def predict_caption(model, image, tokenizer, max_length, features):\n",
        "    feature = features[image]\n",
        "    in_text = \"startseq\"\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        y_pred = model.predict([feature, sequence])\n",
        "        y_pred = np.argmax(y_pred)\n",
        "        word = idx_to_word(y_pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += \" \" + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MVco6cTieJIG"
      },
      "outputs": [],
      "source": [
        "# Performing sentiment analysis on captions\n",
        "def analyze_sentiment(caption):\n",
        "    analysis = TextBlob(caption)\n",
        "    return analysis.sentiment.polarity, analysis.sentiment.subjectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iTi8LYleL7U"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generating and displaying captions with sentiment analysis\n",
        "samples = test.sample(15)\n",
        "samples.reset_index(drop=True, inplace=True)\n",
        "for index, record in samples.iterrows():\n",
        "    img = load_img(os.path.join(image_path, record['image']), target_size=(224, 224))\n",
        "    img = img_to_array(img)\n",
        "    img = img / 255.\n",
        "    caption = predict_caption(caption_model, record['image'], tokenizer, max_length, features)\n",
        "    samples.loc[index, 'caption'] = caption\n",
        "    polarity, subjectivity = analyze_sentiment(caption)\n",
        "    samples.loc[index, 'polarity'] = polarity\n",
        "    samples.loc[index, 'subjectivity'] = subjectivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyInbXCBuGvQ"
      },
      "outputs": [],
      "source": [
        "def display_images_with_sentiment(temp_df, image_path):\n",
        "    temp_df = temp_df.reset_index(drop=True)\n",
        "    plt.figure(figsize=(20, 20))\n",
        "    n = 0\n",
        "    for i in range(len(temp_df)):\n",
        "        n += 1\n",
        "        plt.subplot(5, 5, n)\n",
        "        plt.subplots_adjust(hspace=0.7, wspace=0.3)\n",
        "        image_path_full = os.path.join(image_path, temp_df.image[i])\n",
        "        image = readImage(image_path_full)\n",
        "        plt.imshow(image)\n",
        "\n",
        "        caption = clean_caption(temp_df.caption[i])\n",
        "        sentiment = TextBlob(caption).sentiment\n",
        "        polarity = sentiment.polarity\n",
        "        subjectivity = sentiment.subjectivity\n",
        "\n",
        "        # Print caption and sentiment analysis results for each image\n",
        "        print(f\"Caption: {caption}\")\n",
        "        print(f\"Polarity: {polarity:.2f}, Subjectivity: {subjectivity:.2f}\")\n",
        "\n",
        "        # Determine sentiment label based on polarity\n",
        "        if polarity > 0:\n",
        "            sentiment_label = \"Positive\"\n",
        "        elif polarity < 0:\n",
        "            sentiment_label = \"Negative\"\n",
        "        else:\n",
        "            sentiment_label = \"Neutral\"\n",
        "\n",
        "        print(f\"Sentiment: {sentiment_label}\\n\")\n",
        "\n",
        "        title = f\"{caption}\\nPolarity: {polarity:.2f}, Subjectivity: {subjectivity:.2f}\"\n",
        "        plt.title(\"\\n\".join(wrap(title, 40)))\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout(pad=2.0)\n",
        "\n",
        "# Example usage:\n",
        "display_images_with_sentiment(samples, '/content/Images')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}